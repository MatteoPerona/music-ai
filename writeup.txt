Task 1: Composer Classification

For Task 1, I improved upon the baseline by implementing a more sophisticated feature extraction system and using a Random Forest classifier. The baseline used only average pitch and duration, while my solution extracts 15 musical features including rhythm patterns (time differences between notes), pitch statistics (variety, most common pitches), and duration patterns (variety, most common durations). These features better capture the unique characteristics of each composer's style. The Random Forest classifier (200 trees, max depth 10) was chosen for its ability to handle the increased feature complexity while preventing overfitting. This approach significantly improved upon the baseline accuracy by capturing more nuanced musical patterns that distinguish between different composers.

Task 2: Next Sequence Prediction

For Task 2, I developed a machine learning approach that goes beyond the baseline's simple pitch similarity comparison. The solution extracts 10 features from each sequence (20 total when combined) including pitch statistics (mean, standard deviation, range, and variety of pitches), duration patterns (mean and standard deviation of note lengths), and rhythm features (statistics of time differences between notes). These features are fed into a Random Forest classifier (200 trees, max depth 10) that learns to identify whether two sequences are neighbors in a real piece. The model considers both melodic and rhythmic patterns, capturing the musical coherence between adjacent sequences. This approach improves upon the baseline by learning complex patterns in how musical sequences naturally flow into each other, rather than relying on a single feature comparison. The model also includes sequence-level features like the total number of notes and duration in beats to better understand the musical context.

Task 3: Music Tagging

For Task 3, I enhanced the baseline CNN architecture with several improvements to better handle the multilabel classification task. The model uses a deeper network with three convolutional layers (32, 64, and 128 filters) and batch normalization to improve training stability. The mel spectrogram representation is enhanced with 128 mel bands (up from 64 in the baseline) to capture more detailed frequency information. The model includes dropout (0.5) and a larger fully connected layer (512 units) to prevent overfitting while maintaining the capacity to learn complex patterns. The training process uses validation-based early stopping and model checkpointing to save the best model based on validation mAP. This approach improves upon the baseline by better capturing the spectral characteristics that distinguish different music genres and styles, while the multilabel classification framework effectively handles the multiple tags per audio file.

